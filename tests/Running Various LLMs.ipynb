{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a3c1e4-2385-4de6-af55-1f728ca8bc9e",
   "metadata": {},
   "source": [
    "**Notas de uso de distintos LLMs**\n",
    "\n",
    "Llama 3 funciona normal\n",
    "\n",
    "Gemma 3 funciona normal\n",
    "\n",
    "GptOSS tiene pedos con la versión actual de transformers\n",
    "\n",
    "No se probaron más"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a43bf363-c063-4221-b6b6-66d6f8e6408d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `gpt_oss` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1218\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1218\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:914\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m--> 914\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    915\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'gpt_oss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m openai_quant_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, bnb_4bit_compute_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[0;32m      9\u001b[0m openai_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai/gpt-oss-20b\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m openai \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopenai/gpt-oss-20b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(dev)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:547\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    545\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 547\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    548\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    549\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    550\u001b[0m     code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[0;32m    551\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    554\u001b[0m )\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1220\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1218\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m-> 1220\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1221\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1222\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1225\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not work, and the checkpoint is very new, then there may not be a release version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1226\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers from source with the command \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1228\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1229\u001b[0m         )\n\u001b[0;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `gpt_oss` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "openai_genconfig = GenerationConfig.from_pretrained('openai/gpt-oss-20b')\n",
    "openai_quant_config = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = torch.bfloat16)\n",
    "openai_tokenizer = AutoTokenizer.from_pretrained('openai/gpt-oss-20b')\n",
    "openai = AutoModelForCausalLM.from_pretrained('openai/gpt-oss-20b', torch_dtype = 'auto').to(dev) \n",
    "#openai = AutoModelForCausalLM.from_pretrained('openai/gpt-oss-20b', quantization_config = openai_quant_config).to(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02da5b3-1c6f-4772-a9e1-11a3433c1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_gen_config = GenerationConfig.from_pretrained(\"google/gemma-3-270m\")\n",
    "gemma_quant_config = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = torch.bfloat16)\n",
    "\n",
    "gemma3_270_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "gemma3_270 = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\", quantization_config = gemma_quant_config).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7baaee-2e24-4eba-9372-8271491d3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "kuro_ds = load_dataset('Kurosawama/Translation_DPO_Llama-3.1-8B', split='train')\n",
    "text = kuro_ds['chosen'][5][0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca9697-ea82-4dfc-a27b-6468fdb44027",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma1b_gen_config = GenerationConfig.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "gemma1b_quant_config = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = torch.bfloat16)\n",
    "\n",
    "gemma1b_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "gemma1b = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\", quantization_config = gemma1b_quant_config).to(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c37f18a-0b83-48c8-b5cf-5b839cfd182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(text_input, model, tokenizer):\n",
    "    llm_input = tokenizer(text_input, return_tensors = 'pt', padding = \"longest\", pad_to_multiple_of = 8).to(dev)\n",
    "    outputs = model.generate(**llm_input, max_new_tokens = 200)\n",
    "    answer = tokenizer.batch_decode(outputs, skip_special_tokens = True)[0]\n",
    "    answer = answer[len(llm_input):]\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07b7571-0076-4ba5-a98a-54e9f46acc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Given a problem description and a question, the task is to parse the problem and the question into first-order logic formulars.\n",
      "    The grammar of the first-order logic formular is defined as follows:\n",
      "    1) logical conjunction of expr1 and expr2: expr1 ∧ expr2\n",
      "    2) logical disjunction of expr1 and expr2: expr1 ∨ expr2\n",
      "    3) logical exclusive disjunction of expr1 and expr2: expr1 ⊕ expr2\n",
      "    4) logical negation of expr1: ¬expr1\n",
      "    5) expr1 implies expr2: expr1 → expr2\n",
      "    6) expr1 if and only if expr2: expr1 ↔ expr2\n",
      "    7) logical universal quantification: ∀x\n",
      "    8) logical existential quantification: ∃x\n",
      "    A detailed example is shown next. Only answer with the premises, omit the explanation.\n",
      "    --------------\n",
      "    Problem:\n",
      "    All people who regularly drink coffee are dependent on caffeine. People either regularly drink coffee or joke about being addicted to caffeine. No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
      "    Predicates:\n",
      "    Dependent(x) ::: x is a person dependent on caffeine.\n",
      "    Drinks(x) ::: x regularly drinks coffee.\n",
      "    Jokes(x) ::: x jokes about being addicted to caffeine.\n",
      "    Unaware(x) ::: x is unaware that caffeine is a drug.\n",
      "    Student(x) ::: x is a student.\n",
      "    Premises:\n",
      "    ∀x (Drinks(x) → Dependent(x)) ::: All people who regularly drink coffee are dependent on caffeine.\n",
      "    ∀x (Drinks(x) ⊕ Jokes(x)) ::: People either regularly drink coffee or joke about being addicted to caffeine.\n",
      "    ∀x (Jokes(x) → ¬Unaware(x)) ::: No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. \n",
      "    (Student(rina) ∧ Unaware(rina)) ⊕ ¬(Student(rina) ∨ Unaware(rina)) ::: Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. \n",
      "    ¬(Dependent(rina) ∧ Student(rina)) → (Dependent(rina) ∧ Student(rina)) ⊕ ¬(Dependent(rina) ∨ Student(rina)) ::: If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
      "    --------------\n",
      "    \n",
      "    Problem:\n",
      "    ['Miroslav Venhoda was a Czech choral conductor who specialized in the performance of Renaissance and Baroque music.', 'Any choral conductor is a musician.', 'Some musicians love music.', 'Miroslav Venhoda published a book in 1946 called Method of Studying Gregorian Chant.']\n",
      "    Predicates:\n",
      "    ChoralConductor(x) :: x is a choral conductor.\n",
      "    Music(x) :: x loves music.\n",
      "    Run(x) :: x publishes a book.\n",
      "    Premises:\n",
      "    ∀x (ChoralConductor(x) → Music(x)) ::: Any choral conductor loves music.\n",
      "    ∀x (Music(x) →  Music(x) ⊕  ChoralConductor(x)) ::: Some musicians love music.\n",
      "    ∀x (ChoralConductor(x) ⊕ Music(x)) ↔ (ChoralConductor(x) ∧ Music(x)) ⊕ (ChoralConductor(x) ∨ Music(x))\n",
      "    And now, explain the logical entailment in the statement, \"If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a\n"
     ]
    }
   ],
   "source": [
    "generation(text, gemma1b, gemma1b_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744887cf-9194-48fa-83d7-15a4c513e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee33a7f1-84c0-4563-928b-a745df077f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836\n"
     ]
    }
   ],
   "source": [
    "test = \"\"\"\n",
    "∀x (Drinks(x) → Dependent(x)) ::: All people who regularly drink coffee are dependent on caffeine.\n",
    "    ∀x (Drinks(x) ⊕ Jokes(x)) ::: People either regularly drink coffee or joke about being addicted to caffeine.\n",
    "    ∀x (Jokes(x) → ¬Unaware(x)) ::: No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. \n",
    "    (Student(rina) ∧ Unaware(rina)) ⊕ ¬(Student(rina) ∨ Unaware(rina)) ::: Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. \n",
    "    ¬(Dependent(rina) ∧ Student(rina)) → (Dependent(rina) ∧ Student(rina)) ⊕ ¬(Dependent(rina) ∨ Student(rina)) ::: If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
    "    \"\"\"\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3f957-0750-4486-a736-c32ff4422664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

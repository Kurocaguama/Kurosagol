{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43bf363-c063-4221-b6b6-66d6f8e6408d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dde85b70fbd4475aa779fbaad15b04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5502b1fb15104eef98729bbff8e78bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gemma1b_gen_config = GenerationConfig.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "gemma1b_quant_config = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = torch.bfloat16)\n",
    "\n",
    "gemma1b_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "gemma1b = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\", quantization_config = gemma1b_quant_config).to(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02da5b3-1c6f-4772-a9e1-11a3433c1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_gen_config = GenerationConfig.from_pretrained(\"google/gemma-3-270m\")\n",
    "gemma_quant_config = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = torch.bfloat16)\n",
    "\n",
    "gemma3_270_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "gemma3_270 = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\", quantization_config = gemma_quant_config).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7baaee-2e24-4eba-9372-8271491d3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "kuro_ds = load_dataset('Kurosawama/Translation_DPO_Llama-3.1-8B', split='train')\n",
    "text = kuro_ds['chosen'][5][0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c37f18a-0b83-48c8-b5cf-5b839cfd182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(text_input, model, tokenizer):\n",
    "    llm_input = tokenizer(text_input, return_tensors = 'pt').to(dev)\n",
    "    outputs = model.generate(**llm_input, max_new_tokens = 250)\n",
    "    answer = tokenizer.batch_decode(outputs, skip_special_tokens = True)[0]\n",
    "    answer = answer[len(llm_input):]\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e07b7571-0076-4ba5-a98a-54e9f46acc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Given a problem description and a question, the task is to parse the problem and the question into first-order logic formulars.\n",
      "    The grammar of the first-order logic formular is defined as follows:\n",
      "    1) logical conjunction of expr1 and expr2: expr1 ∧ expr2\n",
      "    2) logical disjunction of expr1 and expr2: expr1 ∨ expr2\n",
      "    3) logical exclusive disjunction of expr1 and expr2: expr1 ⊕ expr2\n",
      "    4) logical negation of expr1: ¬expr1\n",
      "    5) expr1 implies expr2: expr1 → expr2\n",
      "    6) expr1 if and only if expr2: expr1 ↔ expr2\n",
      "    7) logical universal quantification: ∀x\n",
      "    8) logical existential quantification: ∃x\n",
      "    A detailed example is shown next. Only answer with the premises, omit the explanation.\n",
      "    --------------\n",
      "    Problem:\n",
      "    All people who regularly drink coffee are dependent on caffeine. People either regularly drink coffee or joke about being addicted to caffeine. No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
      "    Predicates:\n",
      "    Dependent(x) ::: x is a person dependent on caffeine.\n",
      "    Drinks(x) ::: x regularly drinks coffee.\n",
      "    Jokes(x) ::: x jokes about being addicted to caffeine.\n",
      "    Unaware(x) ::: x is unaware that caffeine is a drug.\n",
      "    Student(x) ::: x is a student.\n",
      "    Premises:\n",
      "    ∀x (Drinks(x) → Dependent(x)) ::: All people who regularly drink coffee are dependent on caffeine.\n",
      "    ∀x (Drinks(x) ⊕ Jokes(x)) ::: People either regularly drink coffee or joke about being addicted to caffeine.\n",
      "    ∀x (Jokes(x) → ¬Unaware(x)) ::: No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. \n",
      "    (Student(rina) ∧ Unaware(rina)) ⊕ ¬(Student(rina) ∨ Unaware(rina)) ::: Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. \n",
      "    ¬(Dependent(rina) ∧ Student(rina)) → (Dependent(rina) ∧ Student(rina)) ⊕ ¬(Dependent(rina) ∨ Student(rina)) ::: If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
      "    --------------\n",
      "    \n",
      "    Problem:\n",
      "    ['Miroslav Venhoda was a Czech choral conductor who specialized in the performance of Renaissance and Baroque music.', 'Any choral conductor is a musician.', 'Some musicians love music.', 'Miroslav Venhoda published a book in 1946 called Method of Studying Gregorian Chant.']\n",
      "    Predicates:\n",
      "    Music(x)\n",
      "    Choral(x)\n",
      "    Conductor(x)\n",
      "    Public(x)\n",
      "\n",
      "    Premises:\n",
      "    Music(Miroslav Venhoda) ::: Miroslav was a Czech choral conductor.\n",
      "    Choral(Miroslav Venhoda) ::: Miroslav Venhoda is a musician.\n",
      "    Conductor(Miroslav Venhoda) ::: Miroslav was a Czech choral conductor.\n",
      "    Public(Miroslav\n"
     ]
    }
   ],
   "source": [
    "generation(text, gemma1b, gemma1b_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "febb41e6-7779-4169-a94a-d6aa98174806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m gemma3_270\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_input, max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      2\u001b[0m answer \u001b[38;5;241m=\u001b[39m gemma3_270_tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m answer \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;28mlen\u001b[39m(llm_input):]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\generation\\utils.py:2463\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[0;32m   2455\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2456\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2457\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2458\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2459\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2460\u001b[0m     )\n\u001b[0;32m   2462\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2463\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2464\u001b[0m         input_ids,\n\u001b[0;32m   2465\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2466\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2467\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2468\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2469\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2470\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2471\u001b[0m     )\n\u001b[0;32m   2473\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2474\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2475\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2476\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2477\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2478\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2479\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2480\u001b[0m     )\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\generation\\utils.py:3474\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3472\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   3473\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[1;32m-> 3474\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   3475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3476\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "llm_input = gemma3_270_tokenizer(text, return_tensors = 'pt').to(dev)\n",
    "outputs = gemma3_270.generate(**llm_input, max_new_tokens = 100)\n",
    "answer = gemma3_270_tokenizer.batch_decode(outputs, skip_special_tokens = True)[0]\n",
    "answer = answer[len(llm_input):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744887cf-9194-48fa-83d7-15a4c513e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee33a7f1-84c0-4563-928b-a745df077f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836\n"
     ]
    }
   ],
   "source": [
    "test = \"\"\"\n",
    "∀x (Drinks(x) → Dependent(x)) ::: All people who regularly drink coffee are dependent on caffeine.\n",
    "    ∀x (Drinks(x) ⊕ Jokes(x)) ::: People either regularly drink coffee or joke about being addicted to caffeine.\n",
    "    ∀x (Jokes(x) → ¬Unaware(x)) ::: No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. \n",
    "    (Student(rina) ∧ Unaware(rina)) ⊕ ¬(Student(rina) ∨ Unaware(rina)) ::: Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. \n",
    "    ¬(Dependent(rina) ∧ Student(rina)) → (Dependent(rina) ∧ Student(rina)) ⊕ ¬(Dependent(rina) ∨ Student(rina)) ::: If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
    "    \"\"\"\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3f957-0750-4486-a736-c32ff4422664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

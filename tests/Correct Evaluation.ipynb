{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b514fa-1016-4c4a-8396-141cfc820769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97e45f72-2cd0-4122-a94e-975e31652d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "\t\"\"\"\n",
    "\t\tPermite evaluar el dataset de las respuestas de los modelos.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, dataset, split):\n",
    "\t\t\"\"\"\n",
    "\t\tdataset = str ;  Nombre del dataset en HuggingFace\n",
    "\t\tsplit = str ; Split a trabajar.\n",
    "\t\t\"\"\"\n",
    "\t\tself.ds = load_dataset(dataset, split = split) \n",
    "\t\n",
    "\tdef premises_per_answer(self, ds_answer, llm):\n",
    "\t\t\"\"\"\n",
    "\t\t\tLimpia la respuesta de un dataset y extrae las premisas necesarias para calcular LogicSim.\n",
    "\n",
    "\t\t\tllm = Bool ; Señala si se va a evaluar la respuesta generada por un LLM.\n",
    "\t\t\"\"\"\n",
    "\t\tinstance = ds_answer.split('\\n')\n",
    "\t\tif llm:\n",
    "\t\t\tfor i in range(len(instance)):\n",
    "\t\t\t\tinstance[i] = re.sub('(:::)+([ A-z.]+)', '', instance[i])\n",
    "\t\t\t\tinstance[i] = re.sub('(  )+', '', instance[i])\n",
    "\t\t\t\n",
    "\t\tfunction_list = []\n",
    "\t\tfor _ in instance:\n",
    "\t\t\t# La siguiente expresión separa las respuestas en sus premisas.\n",
    "\t\t\taux = re.finditer(r'[A-z]+\\(([A-z]+(,? [A-z]+)*)\\)', _)\n",
    "\t\t\tfor regex in aux:\n",
    "\t\t\t\tfunction_list.append(regex.group())\n",
    "\t\tfunction_set = list(set(function_list))\n",
    "\n",
    "\t\treturn function_list, len(instance), len(function_list), len(function_set)\n",
    "\n",
    "\tdef compare_answers(self, folio, llm):\n",
    "\t\t\"\"\"\n",
    "\t\t\tDadas dos entradas de un mismo dataset (folio[i], llm_ans[i]), extrae y calcula los valores para LogicSim(x,y)\n",
    "\t\t\"\"\"\n",
    "\t\tgs_prem, gs_prem_count, gs_funcs_apps, gs_total_funcs = self.premises_per_answer(folio, False)\n",
    "\t\tllm_prem, llm_prem_count, llm_funcs_apps, llm_total_funcs = self.premises_per_answer(llm, True)\n",
    "\n",
    "\t\t# Operaciones de conjuntos\n",
    "\t\tunion_prem = len(list(set(gs_prem).union(set(llm_prem))))\n",
    "\t\tintersection_prem = len(list(set(gs_prem).intersection(set(llm_prem))))\n",
    "\t\tiou = intersection_prem / union_prem\n",
    "\n",
    "\t\t# Valores absolutos\n",
    "\t\tprem_dif = abs(gs_prem_count - llm_prem_count)\n",
    "\t\tfunc_apps_dif = abs(gs_funcs_apps - llm_funcs_apps)\n",
    "\t\tfunc_total_dif = abs(gs_total_funcs - llm_total_funcs)\n",
    "\n",
    "\t\tlogicsim = round(iou + prem_dif + func_apps_dif + func_total_dif, 2)\n",
    "\t\t#print(logicsim)\n",
    "\t\treturn logicsim\n",
    "\n",
    "\tdef logic_sim(self, column_name):\n",
    "\t\t\"\"\"\n",
    "\t\t\tCalcula LogicSim(x,y) entre x = FOLIO_answer, y = LLM_answer.\n",
    "\n",
    "\t\t\tcolumn_name = str ; El nombre de la columna donde se almacenan las respuestas de los LLMs.\n",
    "\t\t\"\"\"\n",
    "\t\tfolio_column = self.ds['FOLIO'] # Ya existe una columna del ds que se llama 'FOLIO'.\n",
    "\n",
    "\t\t# OBS: Hay que cambiar esta parte, los nombres de las columnas están muy wack.\n",
    "\t\tllm_column = self.ds[column_name]\n",
    "\t\taverage = 0\n",
    "\n",
    "\t\tfor i in range(len(folio_column)):\n",
    "\t\t\taverage += self.compare_answers(folio_column[i], llm_column[i])\n",
    "\t\tprint(\"LogicSim promedio: {}\".format(round(average/len(folio_column), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4093e34b-c037-4b9a-981d-66361b9ca501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicSim promedio: 20.443743842364533\n"
     ]
    }
   ],
   "source": [
    "a = Evaluation('Kurosawama/EVAL_Llama-3.1-8B', 'trans')\n",
    "column = list(a.ds.features.keys())[1]\n",
    "a.logic_sim(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "966bf635-d56d-4a28-860d-3557856401e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Modelo: Kurosawama/EVAL_gemma-3-1b-it\n",
      "LogicSim promedio: 29.08\n",
      "LogicSim promedio: 46.7\n",
      "=====================\n",
      "Modelo: Kurosawama/EVAL_Llama-3.2-3B\n",
      "LogicSim promedio: 24.37\n",
      "LogicSim promedio: 47.45\n",
      "=====================\n",
      "Modelo: Kurosawama/EVAL_Llama-3.1-8B\n",
      "LogicSim promedio: 20.44\n",
      "LogicSim promedio: 47.02\n",
      "=====================\n",
      "Modelo: Kurosawama/EVAL_Llama-3.2-3B-Instruct\n",
      "LogicSim promedio: 25.76\n",
      "LogicSim promedio: 56.58\n",
      "=====================\n",
      "Modelo: Kurosawama/EVAL_Llama-3.1-8B-Instruct\n",
      "LogicSim promedio: 22.86\n",
      "LogicSim promedio: 58.52\n"
     ]
    }
   ],
   "source": [
    "def trans_logic_sim(ds_name):\n",
    "    aux = Evaluation(ds_name, 'trans')\n",
    "    column = list(aux.ds.features.keys())[1]\n",
    "    aux.logic_sim(column)\n",
    "    #----\n",
    "    aux1 = Evaluation(ds_name, 'inference')\n",
    "    column = list(aux.ds.features.keys())[1]\n",
    "    aux1.logic_sim(column)\n",
    "\n",
    "dataset_name = [\n",
    "    'Kurosawama/EVAL_gemma-3-1b-it',\n",
    "    'Kurosawama/EVAL_Llama-3.2-3B',\n",
    "    'Kurosawama/EVAL_Llama-3.1-8B',\n",
    "    'Kurosawama/EVAL_Llama-3.2-3B-Instruct',\n",
    "    'Kurosawama/EVAL_Llama-3.1-8B-Instruct'\n",
    "]\n",
    "\n",
    "for _ in dataset_name:\n",
    "    print(\"=====================\")\n",
    "    print(\"Modelo: {}\".format(_))\n",
    "    trans_logic_sim(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ebbe96-a340-4795-b4f2-21f19cf8a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo de arriba es de los modelos -LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba46a2a-1a8c-454f-bddd-332988bfb0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = [\n",
    "    'Kurosawama/EVAL_gemma-3-1b-it_BASE',\n",
    "    'Kurosawama/EVAL_Llama-3.2-3B_BASE',\n",
    "    'Kurosawama/EVAL_Llama-3.1-8B_BASE',\n",
    "    'Kurosawama/EVAL_Llama-3.2-3B-Instruct_BASE',\n",
    "    'Kurosawama/EVAL_Llama-3.1-8B-Instruct_BASE'\n",
    "]\n",
    "\n",
    "for _ in dataset_name:\n",
    "    print(\"=====================\")\n",
    "    print(\"Modelo: {}\".format(_))\n",
    "    trans_logic_sim(_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

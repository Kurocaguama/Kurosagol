{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d15867-e102-4c4c-9914-2dcee5806639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, huggingface_hub, trl\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, BitsAndBytesConfig\n",
    "from peft import LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66339877-e56d-4c3b-a9a6-a9d01c58bab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16.1\n"
     ]
    }
   ],
   "source": [
    "print(trl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1b0f9-5b03-45f5-abac-7a2945d50072",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-3.1-8B'\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    "    inference_mode = False,\n",
    "    r = 8,\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.1\n",
    ")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = torch.bfloat16)\n",
    "gen_config = GenerationConfig.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.chat_template = \"\"\"\n",
    "    <|im_start|>system\n",
    "    {SYSTEM}<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {INPUT}<|im_ed|>\n",
    "    <|im_start|>assistant\n",
    "    {OUTPUT}<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir = '/media/discoexterno/francisco/modelos', quantization_config = quant_config, generation_config = generation_config).to(dev)\n",
    "model.add_adapter(lora_config, adapter_name = 'lora_v1')\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Dataset\n",
    "beam_search = load_dataset('Kurosawama/Translation_DPO_Llama-3.1-8B', split = 'train')\n",
    "\n",
    "# DPO args\n",
    "training_args = DPOConfig(output_dir = 'media/discoexterno/francisco', logging_steps = 20)\n",
    "trainer = DPOTrainer(model = model, args = training_args, processing_class = tokenizer, train_dataset = beam_search)\n",
    "trainer.train()\n",
    "\n",
    "model.push_to_hub('Llama-3.1-8B-Translation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

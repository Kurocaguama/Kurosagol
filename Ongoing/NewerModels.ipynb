{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d2bfbf8",
   "metadata": {},
   "source": [
    "**Notas generales.**\n",
    "\n",
    "Modelos:\n",
    "\n",
    "Tanto gpt-oss como qwen hacen uso de la estrategia de generación en formato harmony. Es decir, primero tienen una sección interna de razonamiento (un tanto esquizofrénico) PREVIO a la respuesta global. Entre más ambiguo sea el prompt más larga es esta sección, por eso prompts como `prompt4` tienen tan buenos resultados. El modelo es capaz de inferir que se le está pasando un ejemplo y logra seguir el formato de buena manera.\n",
    "\n",
    "Lo malo de esto es que se tienen que tener más de 1000 tokens de generación ANTES de que el modelo produzca los resultados finales. Gpt-oss ya da resultados buenos en 2000 max_new_tokens y muchas veces no llega al límite de generación. Qwen usa una mayor cantidad de tokens, las primeras respuestas que salen son apartir de x > 4300~4600 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA RTX 5000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch, re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(dev)\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "path = r''\n",
    "dataset = pd.read_json(path, lines = True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68010bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\", device_map = 'cuda')\n",
    "\n",
    "# -------------------------------\n",
    "qwen_quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype= torch.bfloat16)\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-8B')\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen3-8B', quantization_config = qwen_quant_config, device_map = 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc0ac38",
   "metadata": {},
   "source": [
    "### Logic Prompts (Igual para todos los checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da518270",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    Translate the following premises from natural language to first order logic. \n",
    "    PREMISES:\n",
    "    All eels are fish.\n",
    "    No fish are plants.\n",
    "    Everything displayed in the collection is either a plant or an animal.\n",
    "    All multicellular animals are not bacteria.\n",
    "    All animals displayed in the collection are multicellular.\n",
    "    A sea eel is displayed in the collection.\n",
    "    The sea eel is an eel or an animal or not a plant.\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = \"\"\"\n",
    "    Given a set of premises description and a question, the task is to parse the problem into first-order logic formulars.\n",
    "    --------------\n",
    "    Premises:\n",
    "    All people who regularly drink coffee are dependent on caffeine. \n",
    "    People either regularly drink coffee or joke about being addicted to caffeine. \n",
    "    No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. \n",
    "    Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. \n",
    "    If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
    "    Predicates:\n",
    "    Dependent(x) \n",
    "    Drinks(x) \n",
    "    Jokes(x) \n",
    "    Unaware(x)\n",
    "    Student(x)\n",
    "    Premises:\n",
    "    ∀x (Drinks(x) → Dependent(x)) \n",
    "    ∀x (Drinks(x) ⊕ Jokes(x))\n",
    "    ∀x (Jokes(x) → ¬Unaware(x)) \n",
    "    (Student(rina) ∧ Unaware(rina)) ⊕ ¬(Student(rina) ∨ Unaware(rina)) \n",
    "    ¬(Dependent(rina) ∧ Student(rina)) → (Dependent(rina) ∧ Student(rina)) ⊕ ¬(Dependent(rina) ∨ Student(rina)) \n",
    "    --------------\n",
    "    \n",
    "    Premises:\n",
    "    All eels are fish.\n",
    "    No fish are plants.\n",
    "    Everything displayed in the collection is either a plant or an animal.\n",
    "    All multicellular animals are not bacteria.\n",
    "    All animals displayed in the collection are multicellular.\n",
    "    A sea eel is displayed in the collection.\n",
    "    The sea eel is an eel or an animal or not a plant.\n",
    "    Predicates:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt3 = \"\"\"\n",
    "    Given a set of premises, the task is to parse the problem and the question into first-order logic formulars.\n",
    "    The grammar of the first-order logic formular is defined as follows:\n",
    "    1) logical conjunction of expr1 and expr2: expr1 ∧ expr2\n",
    "    2) logical disjunction of expr1 and expr2: expr1 ∨ expr2\n",
    "    3) logical exclusive disjunction of expr1 and expr2: expr1 ⊕ expr2\n",
    "    4) logical negation of expr1: ¬expr1\n",
    "    5) expr1 implies expr2: expr1 → expr2\n",
    "    6) expr1 if and only if expr2: expr1 ↔ expr2\n",
    "    7) logical universal quantification: ∀x\n",
    "    8) logical existential quantification: ∃x\n",
    "    --------------\n",
    "    Premises:\n",
    "    All people who regularly drink coffee are dependent on caffeine. People either regularly drink coffee or joke about being addicted to caffeine. No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
    "    Predicates:\n",
    "    Dependent(x) ::: x is a person dependent on caffeine.\n",
    "    Drinks(x) ::: x regularly drinks coffee.\n",
    "    Jokes(x) ::: x jokes about being addicted to caffeine.\n",
    "    Unaware(x) ::: x is unaware that caffeine is a drug.\n",
    "    Student(x) ::: x is a student.\n",
    "    Premises:\n",
    "    ∀x (Drinks(x) → Dependent(x)) ::: All people who regularly drink coffee are dependent on caffeine.\n",
    "    ∀x (Drinks(x) ⊕ Jokes(x)) ::: People either regularly drink coffee or joke about being addicted to caffeine.\n",
    "    ∀x (Jokes(x) → ¬Unaware(x)) ::: No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. \n",
    "    (Student(rina) ∧ Unaware(rina)) ⊕ ¬(Student(rina) ∨ Unaware(rina)) ::: Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. \n",
    "    ¬(Dependent(rina) ∧ Student(rina)) → (Dependent(rina) ∧ Student(rina)) ⊕ ¬(Dependent(rina) ∨ Student(rina)) ::: If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
    "    --------------\n",
    "    \n",
    "    Premises:\n",
    "    All eels are fish.\n",
    "    No fish are plants.\n",
    "    Everything displayed in the collection is either a plant or an animal.\n",
    "    All multicellular animals are not bacteria.\n",
    "    All animals displayed in the collection are multicellular.\n",
    "    A sea eel is displayed in the collection.\n",
    "    The sea eel is an eel or an animal or not a plant.\n",
    "    Predicates:\n",
    "\"\"\"\n",
    "\n",
    "final_prompt = \"\"\"\n",
    "    Given a set of premises, the task is to parse the problem and the question into first-order logic formulars. Answer only with the translated premises.\n",
    "    The grammar of the first-order logic formular is defined as follows:\n",
    "    1) logical conjunction of expr1 and expr2: expr1 ∧ expr2\n",
    "    2) logical disjunction of expr1 and expr2: expr1 ∨ expr2\n",
    "    3) logical exclusive disjunction of expr1 and expr2: expr1 ⊕ expr2\n",
    "    4) logical negation of expr1: ¬expr1\n",
    "    5) expr1 implies expr2: expr1 → expr2\n",
    "    6) expr1 if and only if expr2: expr1 ↔ expr2\n",
    "    7) logical universal quantification: ∀x\n",
    "    8) logical existential quantification: ∃x\n",
    "    --------------\n",
    "    Natural Language Premises:\n",
    "    All people who regularly drink coffee are dependent on caffeine. People either regularly drink coffee or joke about being addicted to caffeine. No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
    "    Predicates:\n",
    "    Dependent(x) ::: x is a person dependent on caffeine.\n",
    "    Drinks(x) ::: x regularly drinks coffee.\n",
    "    Jokes(x) ::: x jokes about being addicted to caffeine.\n",
    "    Unaware(x) ::: x is unaware that caffeine is a drug.\n",
    "    Student(x) ::: x is a student.\n",
    "    FOL Premises:\n",
    "    ∀x (Drinks(x) → Dependent(x)) ::: All people who regularly drink coffee are dependent on caffeine.\n",
    "    ∀x (Drinks(x) ⊕ Jokes(x)) ::: People either regularly drink coffee or joke about being addicted to caffeine.\n",
    "    ∀x (Jokes(x) → ¬Unaware(x)) ::: No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. \n",
    "    (Student(rina) ∧ Unaware(rina)) ⊕ ¬(Student(rina) ∨ Unaware(rina)) ::: Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. \n",
    "    ¬(Dependent(rina) ∧ Student(rina)) → (Dependent(rina) ∧ Student(rina)) ⊕ ¬(Dependent(rina) ∨ Student(rina)) ::: If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
    "    --------------\n",
    "    \n",
    "    Natural Language Premises:\n",
    "    {}\n",
    "    FOL Premises:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d06897",
   "metadata": {},
   "source": [
    "### Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_oss(text):\n",
    "\n",
    "    message = [\n",
    "    {\"role\": \"system\", \"content\": \"Reasoning: high.\"},\n",
    "    {\"role\": \"developer\", \"content\": \"You translate natural language premises to first-order-logic formulars. Answer only with the translated premises.\"},\n",
    "    {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    inputs = gpt_tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        add_generation_prompt = False,\n",
    "        tokenize = True,\n",
    "        return_dict = True,\n",
    "        return_tensors = \"pt\"\n",
    "    ).to(gpt_model.device)\n",
    "\n",
    "    outputs = gpt_model.generate(**inputs, max_new_tokens = 2000) \n",
    "    # 50, 200, 350, 600, 1000 no.\n",
    "    answer = gpt_tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:-10])\n",
    "    try:\n",
    "        aux = re.search(r'(final<\\|message\\|>)', answer)\n",
    "        return answer[aux.end():]\n",
    "    except:\n",
    "        print(\"No hay token de inicio del mensaje final.\")\n",
    "\n",
    "\n",
    "def chat_qwen(msg, thinking):\n",
    "    \"\"\"\n",
    "        mensajes = list ; lista con el diccionario de inputs.\n",
    "        thinking = bool ; valor booleano que determina si el modelo usa \"thinking mode\" xd\n",
    "    \"\"\"\n",
    "    text = qwen_tokenizer.apply_chat_template(\n",
    "    msg,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=thinking \n",
    "    )\n",
    "    inputs = qwen_tokenizer([text], return_tensors='pt').to(dev)\n",
    "    gen_ids = qwen_model.generate(**inputs, max_new_tokens = 8000) \n",
    "    # 350, 750, 1250, 1500, 2000, 4000. No </think>\n",
    "    # Con 8000 apenas vimos el primter <\\think> token nmms XD.\n",
    "    output_ids = gen_ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "\n",
    "    try:\n",
    "        index = len(gen_ids) - gen_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    norm_cont = qwen_tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\") \n",
    "    \n",
    "    regex = re.search(r'(</think>)', norm_cont)\n",
    "    try:\n",
    "        return norm_cont[regex.end():] \n",
    "    except:\n",
    "        print(\"No <\\\\think> token found.\")\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================================================\n",
    "# ====================================================================================\n",
    "# ====================================================================================\n",
    "# ====================================================================================\n",
    "\n",
    "\n",
    "\n",
    "def translations(model):\n",
    "    \"\"\"\n",
    "    Genera el conjunto de datos de traducción de FOLIO.\n",
    "    \"\"\"\n",
    "    if model == 'gpt':\n",
    "        message = [\n",
    "        {\"role\": \"system\", \"content\": \"Reasoning: high.\"},\n",
    "        {\"role\": \"developer\", \"content\": \"You translate natural language premises to first-order-logic formulars. Answer only with the translated premises.\"},\n",
    "        {\"role\": \"user\", \"content\": ''}\n",
    "        ]\n",
    "    else:\n",
    "        message = [\n",
    "            {\"role\": \"user\", \"content\": ''}\n",
    "        ]\n",
    "\n",
    "    translation = []\n",
    "    for _ in dataset[\"premises\"]:\n",
    "        aux_prompt = final_prompt.format(_)\n",
    "        message[-1]['content'] = aux_prompt\n",
    "        if model == 'gpt':\n",
    "            answer = chat_oss(message)\n",
    "        else:\n",
    "            answer = chat_qwen(message)\n",
    "\n",
    "        translation.append(answer)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_translations = translations('gpt')\n",
    "qwen_translations =translations('qwen')\n",
    "\n",
    "dataset['gpt Translations'] = gpt_translations\n",
    "dataset['qwen Translations'] = qwen_translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb29c14",
   "metadata": {},
   "source": [
    "### gpt-oss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68dfc80",
   "metadata": {},
   "source": [
    "#### Lógica Finalmente (ESTÁ CHIDO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed59b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: torch.Size([2168])\n",
      "===============================\n",
      "**Predicates**\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| `Eel(x)`        | *x* is an eel |\n",
      "| `Fish(x)`       | *x* is a fish |\n",
      "\n",
      "===================================================================================\n",
      "===================================================================================\n",
      "===================================================================================\n",
      "Outputs: torch.Size([2464])\n",
      "===============================\n",
      "No hay token de inicio del mensaje final.\n",
      "===================================================================================\n",
      "===================================================================================\n",
      "===================================================================================\n",
      "Outputs: torch.Size([2435])\n",
      "===============================\n",
      "**First‑order‑logic representation of the second set of premises**\n",
      "\n",
      "Let the following predicates be used (constants are written in lower‑case letters in parentheses):\n",
      "\n",
      "| Predicate | Meaning |\n",
      "|-----------|---------|\n",
      "| `Eel(x)` | *x* is an eel |\n",
      "| `Fish(x)` | *x* is a fish |\n",
      "| `Plant(x)` | *x* is a plant |\n",
      "| `Animal(x)` | *x* is an animal |\n",
      "| `Multicellular(x)` | *x* is multicellular |\n",
      "| `Bacteria(x)` | *x* is bacteria |\n",
      "| `Displayed(x)` | *x* is displayed in the collection |\n",
      "\n",
      "Let the constant **`seaEel`** denote the particular sea eel mentioned in the\n",
      "text.\n",
      "\n",
      "Then the premises translate into the following first‑order‑logic formulas:\n",
      "\n",
      "1. **All eels are fish**  \n",
      "   \\[\n",
      "   \\forall x\\; (\\,\\text{Eel}(x) \\;\\rightarrow\\; \\text{Fish}(x)\\,)\n",
      "   \\]\n",
      "\n",
      "2. **No fish are plants**  \n",
      "   \\[\n",
      "   \\forall x\\; (\\,\\text{Fish}(x) \\;\\rightarrow\\; \\neg \\text{Plant}(x)\\,)\n",
      "   \\]\n",
      "\n",
      "3. **Everything displayed in the collection is either a plant or an animal**  \n",
      "   \\[\n",
      "   \\forall x\\; (\\,\\text{Displayed}(x) \\;\\rightarrow\\; (\\,\\text{Plant}(x) \\;\\lor\\; \\text{Animal}(x)\\,)\\,)\n",
      "   \\]\n",
      "\n",
      "4. **All multicellular animals are not bacteria**  \n",
      "   \\[\n",
      "   \\forall x\\; (\\,(\\text{Multicellular}(x)\\;\\land\\;\\text{Animal}(x))\n",
      "             \\;\\rightarrow\\; \\neg \\text{Bacteria}(x)\\,)\n",
      "   \\]\n",
      "\n",
      "5. **All animals displayed in the collection are multicellular**  \n",
      "   \\[\n",
      "   \\forall x\\; (\\,(\\text{Animal}(x)\\;\\land\\;\\text{Displayed}(x))\n",
      "             \\;\\rightarrow\\; \\text{Multicellular}(x)\\,)\n",
      "   \\]\n",
      "\n",
      "6. **A sea eel is displayed in the collection**  \n",
      "   \\[\n",
      "   \\text{Displayed}(\\text{seaEel})\n",
      "   \\]\n",
      "\n",
      "7. **The sea eel is an eel or an animal or not a plant**  \n",
      "   \\[\n",
      "   \\text{Eel}(\\text{seaEel}) \\;\\lor\\; \\text{Animal}(\\text{seaEel}) \\;\\lor\\; \\neg \\text{Plant}(\\text{seaEel})\n",
      "   \\]\n",
      "\n",
      "These seven formulas capture all the given premises in\n",
      "===================================================================================\n",
      "===================================================================================\n",
      "===================================================================================\n",
      "Outputs: torch.Size([2601])\n",
      "===============================\n",
      "∃x (Eel(x) ∧ Displayed(x) ∧ Sealeel(x))  \n",
      "∀x (Eel(x) → Fish(x))  \n",
      "∀x (Fish(x) → ¬Plant(x))  \n",
      "∀x (Displayed(x) → (Plant(x) ⊕ Animal(x)))  \n",
      "∀x ((Animal(x) ∧ Multicellular(x)) → ¬Bacteria(x))  \n",
      "∀x ((Animal(x) ∧ Displayed(x)) → Multicellular(x))  \n",
      "Eel(sealeel) ∧ Displayed(sealeel)  \n",
      "Eel(sealeel) ∨ Animal(sealeel)\n",
      "===================================================================================\n",
      "===================================================================================\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "template_n = [\n",
    "    {\"role\": \"system\", \"content\": \"Reasoning: high.\"},\n",
    "    {\"role\": \"developer\", \"content\": \"You translate natural language premises to first-order-logic formulars. Answer only with the translated premises.\"},\n",
    "    {\"role\": \"user\", \"content\": ''}\n",
    "]\n",
    "\n",
    "for texto in prompt_list:\n",
    "    template_n[2]['content'] = texto\n",
    "    chat_oss(template_n)\n",
    "    print(\"===================================================================================\")\n",
    "    print(\"===================================================================================\")\n",
    "    print(\"===================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efb3154",
   "metadata": {},
   "source": [
    "### Qwen3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f04f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_qwen(msg, thinking):\n",
    "    \"\"\"\n",
    "        mensajes = list ; lista con el diccionario de inputs.\n",
    "        thinking = bool ; valor booleano que determina si el modelo usa \"thinking mode\" xd\n",
    "    \"\"\"\n",
    "    text = qwen_tokenizer.apply_chat_template(\n",
    "    msg,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=thinking \n",
    "    )\n",
    "    inputs = qwen_tokenizer([text], return_tensors='pt').to(dev)\n",
    "    gen_ids = qwen_model.generate(**inputs, max_new_tokens = 8000) \n",
    "    # 350, 750, 1250, 1500, 2000, 4000. No </think>\n",
    "    # Con 8000 apenas vimos el primter <\\think> token nmms XD.\n",
    "    output_ids = gen_ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "    print(len(output_ids))\n",
    "\n",
    "    try:\n",
    "        index = len(gen_ids) - gen_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    #thinking_content = qwen_tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    norm_cont = qwen_tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\") \n",
    "    \n",
    "    regex = re.search(r'(</think>)', norm_cont)\n",
    "    try:\n",
    "        #print(\"Normal: \", norm_cont[regex.end():])  \n",
    "        return norm_cont[regex.end():] \n",
    "    except:\n",
    "        print(\"No <\\\\think> token found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e398172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt1\n",
      "5789\n",
      "Normal:  \n",
      "\n",
      "Here is the translation of the premises into first-order logic:\n",
      "\n",
      "1. **All eels are fish.**  \n",
      "   $ \\forall x \\, (\\text{Eel}(x) \\rightarrow \\text{Fish}(x)) $\n",
      "\n",
      "2. **No fish are plants.**  \n",
      "   $ \\forall x \\, (\\text{Fish}(x) \\rightarrow \\neg \\text{Plant}(x)) $\n",
      "\n",
      "3. **Everything displayed in the collection is either a plant or an animal.**  \n",
      "   $ \\forall x \\, (\\text{Displayed}(x) \\rightarrow \\text{Plant}(x) \\vee \\text{Animal}(x)) $\n",
      "\n",
      "4. **All multicellular animals are not bacteria.**  \n",
      "   $ \\forall x \\, ((\\text{Animal}(x) \\wedge \\text{Multicellular}(x)) \\rightarrow \\neg \\text{Bacteria}(x)) $\n",
      "\n",
      "5. **All animals displayed in the collection are multicellular.**  \n",
      "   $ \\forall x \\, ((\\text{Animal}(x) \\wedge \\text{Displayed}(x)) \\rightarrow \\text{Multicellular}(x)) $\n",
      "\n",
      "6. **A sea eel is displayed in the collection.**  \n",
      "   $ \\exists x \\, (\\text{SeaEel}(x) \\wedge \\text{Displayed}(x)) $\n",
      "\n",
      "7. **The sea eel is an eel or an animal or not a plant.**  \n",
      "   $ \\exists x \\, (\\text{SeaEel}(x) \\wedge \\text{Displayed}(x) \\wedge (\\text{Eel}(x) \\vee \\text{Animal}(x) \\vee \\neg \\text{Plant}(x))) $\n",
      "\n",
      "---\n",
      "\n",
      "### Explanation of Key Points:\n",
      "- **Premise 7** is translated as an existential statement to ensure consistency with the structure of the other premises. While the disjunction $ \\text{Eel}(x) \\vee \\text{Animal}(x) \\vee \\neg \\text{Plant}(x) $ is tautologically true for a sea eel (since sea eels are eels, which are animals, and not plants), the premise is included explicitly as stated in the problem.  \n",
      "- Quantifiers and logical connectives are used to capture the universal and existential claims in natural language.\n",
      "===========================================================================================\n",
      "===========================================================================================\n",
      "Prompt2\n",
      "8000\n",
      "Normal:  \n",
      "\n",
      "The task is to translate the given premises into first-order logic (FOL) formulas. Here's the breakdown of the premises and their corresponding FOL representations:\n",
      "\n",
      "---\n",
      "\n",
      "### **Premises 1-5 (People, Coffee, Caffeine, Rina)**\n",
      "1. **All people who regularly drink coffee are dependent on caffeine.**  \n",
      "   $$\n",
      "   \\forall x \\, (\\text{Drinks}(x) \\rightarrow \\text{Dependent}(x))\n",
      "   $$\n",
      "\n",
      "2. **People either regularly drink coffee or joke about being addicted to caffeine.**  \n",
      "   $$\n",
      "   \\forall x \\, (\\text{Drinks}(x) \\oplus\n",
      "===========================================================================================\n",
      "===========================================================================================\n",
      "Prompt3\n",
      "5253\n",
      "Normal:  \n",
      "\n",
      "Here's the structured answer based on the provided premises and the specified first-order logic (FOL) grammar:\n",
      "\n",
      "---\n",
      "\n",
      "### **First Set of Premises (People, Coffee, Caffeine)**\n",
      "1. **All people who regularly drink coffee are dependent on caffeine.**  \n",
      "   $$\n",
      "   \\forall x \\, (\\text{Drinks}(x) \\rightarrow \\text{Dependent}(x))\n",
      "   $$\n",
      "\n",
      "2. **People either regularly drink coffee or joke about being addicted to caffeine.**  \n",
      "   $$\n",
      "   \\forall x \\, (\\text{Drinks}(x) \\oplus \\text{Jokes}(x))\n",
      "   $$\n",
      "\n",
      "3. **No one who jokes about being addicted to caffeine is unaware that caffeine is a drug.**  \n",
      "   $$\n",
      "   \\forall x \\, (\\text{Jokes}(x) \\rightarrow \\neg \\text{Unaware}(x))\n",
      "   $$\n",
      "\n",
      "4. **Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug.**  \n",
      "   $$\n",
      "   (\\text{Student}(\\text{rina}) \\land \\text{Unaware}(\\text{rina})) \\oplus \\neg (\\text{Student}(\\text{rina}) \\lor \\text{Unaware}(\\text{rina}))\n",
      "   $$\n",
      "\n",
      "5. **If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.**  \n",
      "   $$\n",
      "   \\neg (\\text{Dependent}(\\text{rina}) \\land \\text{Student}(\\text{rina})) \\rightarrow (\\text{Dependent}(\\text{rina}) \\land \\text{Student}(\\text{rina})) \\oplus \\neg (\\text{Dependent}(\\text{rina}) \\lor \\text{Student}(\\text{rina}))\n",
      "   $$\n",
      "\n",
      "---\n",
      "\n",
      "### **Second Set of Premises (Eels, Fish, Collection)**\n",
      "Assume the following predicates for the second set of premises:\n",
      "- **Eel(x)**: $x$ is an eel.  \n",
      "- **Fish(x)**: $x$ is a fish.  \n",
      "- **Plant(x)**: $x$ is a plant.  \n",
      "- **Animal(x)**: $x$ is an animal.  \n",
      "- **Multicellular(x)**: $x$ is multicellular.  \n",
      "- **Bacteria(x)**: $x$ is a bacterium.  \n",
      "- **Displayed(x)**: $x$ is displayed in the collection.  \n",
      "- **sea_eel**: A constant denoting the sea eel.  \n",
      "\n",
      "1. **All eels are fish.**  \n",
      "   $$\n",
      "   \\forall x \\, (\\text{Eel}(x) \\rightarrow \\text{Fish}(x))\n",
      "   $$\n",
      "\n",
      "2. **No fish are plants.**  \n",
      "   $$\n",
      "   \\forall x \\, (\\text{Fish}(x) \\rightarrow \\neg \\text{Plant}(x))\n",
      "   $$\n",
      "\n",
      "3. **Everything displayed in the collection is either a plant or an animal.**  \n",
      "   $$\n",
      "   \\forall x \\, (\\text{Displayed}(x) \\rightarrow \\text{Plant}(x) \\lor \\text{Animal}(x))\n",
      "   $$\n",
      "\n",
      "4. **All multicellular animals are not bacteria.**  \n",
      "   $$\n",
      "   \\forall x \\, (\\text{Multicellular}(x) \\land \\text{Animal}(x) \\rightarrow \\neg \\text{Bacteria}(x))\n",
      "   $$\n",
      "\n",
      "5. **All animals displayed in the collection are multicellular.**  \n",
      "   $$\n",
      "   \\forall x \\, (\\text{Displayed}(x) \\land \\text{Animal}(x) \\rightarrow \\text{Multicellular}(x))\n",
      "   $$\n",
      "\n",
      "6. **A sea eel is displayed in the collection.**  \n",
      "   $$\n",
      "   \\text{Displayed}(\\text{sea\\_eel})\n",
      "   $$\n",
      "\n",
      "7. **The sea eel is an eel or an animal or not a plant.**  \n",
      "   $$\n",
      "   \\text{Eel}(\\text{sea\\_eel}) \\lor \\text{Animal}(\\text{sea\\_eel}) \\lor \\neg \\text{Plant}(\\text{sea\\_eel})\n",
      "   $$\n",
      "\n",
      "---\n",
      "\n",
      "### **Summary**\n",
      "- The first set of premises is already correctly translated into FOL using the given grammar and predicates.  \n",
      "- The second set of premises is translated using standard predicates and the same FOL grammar.  \n",
      "- The logical structure of the formulas adheres to the specified rules (e.g., exclusive disjunction, quantification, implication).\n",
      "===========================================================================================\n",
      "===========================================================================================\n",
      "Prompt4\n",
      "2222\n",
      "Normal:  \n",
      "\n",
      "∀x (Eel(x) → Fish(x))  \n",
      "∀x (Fish(x) → ¬Plant(x))  \n",
      "∀x (Displayed(x) → Plant(x) ∨ Animal(x))  \n",
      "∀x ((Multicellular(x) ∧ Animal(x)) → ¬Bacteria(x))  \n",
      "∀x ((Animal(x) ∧ Displayed(x)) → Multicellular(x))  \n",
      "Displayed(s)  \n",
      "Eel(s) ∨ Animal(s) ∨ ¬Plant(s)\n",
      "===========================================================================================\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "qwen_msg = [\n",
    "    {\"role\": \"user\", \"content\": ''}\n",
    "]\n",
    "\n",
    "prompt_no = 0\n",
    "for p in prompt_list:\n",
    "    prompt_no += 1\n",
    "    print(\"Prompt{}\".format(prompt_no))\n",
    "    qwen_msg[0][\"content\"] = p\n",
    "    chat_qwen(qwen_msg, True)\n",
    "    print(\"===========================================================================================\")\n",
    "    print(\"===========================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30603039",
   "metadata": {},
   "source": [
    "### Deepseek\n",
    "\n",
    "Nuevamente este modelo hijo de puta pesa 69,000 billones de parámetros y es lo peor del mundo tratar de usarlo desde HuggingFace. Toca usarlo desde otro lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9dea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d03bdbbebe944eab463e0439c5a4c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 185 files:   0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f87fce7392342968898c5f28ee4e8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_deepseek.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105c465746e64170aff15fb48586dda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a561c3d4d8c04a1190ace1108e53ab78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "niah.png:   0%|          | 0.00/108k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87f2d382b4f452a8f9c7b12ae8d2f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "benchmark.png:   0%|          | 0.00/184k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ee8a90494f414187af61f15109847e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE-MODEL: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777e86ada0764681aaf8835880c499eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README_WEIGHTS.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff755193b308410ba86d66fc6c4b297c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d500ceffd9c74155be92aa3ca8a9f196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE-CODE: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9e2fbd968a45fa9ca9ea2f87d45266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_16B.json:   0%|          | 0.00/417 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287ab06913a147758585d7a5d37f4103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_671B.json:   0%|          | 0.00/503 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9016c06af8ea4ae2b9ac25ec100b4342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_236B.json:   0%|          | 0.00/455 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1bc9c971904ded97cf4346fb396ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc8d20767294d328879192761f8b313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fp8_cast_bf16.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c8a3b5b2d84081b37718f38ad68b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generate.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da430b06c92452d8460809ea1532c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "kernel.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896e0c053b024ffbb5c709551f0fd68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-000163.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d995d61b4ad492eaa73f6afa6697c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-000163.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d962940610b4531965c1ba72ba465fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-000163.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d073d5143f104a49b3da6d9db0c84f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-000163.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8d8dce5fac4c9690d1a46de92776a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "requirements.txt:   0%|          | 0.00/66.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00169dc64235475cbcfed39025485272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-000163.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5b62280cf446689838947bd26a430e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c8b47d839b446db3215d383403cf11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-000163.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868f0d793aa64f06a2a9c484ab875d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-000163.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f737f414fa3f44f88ecd64fe472919e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-000163.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e38baf8a5049fe93c3c1a10e4f8ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-000163.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f719dc3f3bc948cc849255e636a4b5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-000163.safetensors:   0%|          | 0.00/4.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lmdeploy import pipeline\n",
    "\n",
    "pipe = pipeline(\"deepseek-ai/DeepSeek-V3\")\n",
    "\n",
    "message = [{\"role\": \"user\", \"content\": \"Tell me who Lionel Messi is.\"}]\n",
    "output = pipe(message)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a003c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
